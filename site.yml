# ----------------------------------------------------------------------
# PHASE 1: PREPARE ALL NODES
# ----------------------------------------------------------------------
- hosts: k8s_cluster
  become: yes
  roles:
    - role: networking
      tags: networking
    - role: k8s_prep
      tags: k8s_prep

# ----------------------------------------------------------------------
# PHASE 2: BOOTSTRAP LEADER (First Node)
# ----------------------------------------------------------------------
- hosts: masters[0]
  become: yes
  tasks:
    - include_role:
        name: networking
        tasks_from: normalize

    - name: Check if Cluster is initialized
      stat: path=/etc/kubernetes/admin.conf
      register: cluster_admin_conf

    - name: "Upload Kubeadm Config"
      template:
        src: roles/k8s_prep/templates/kubeadm-config.yaml.j2
        dest: /etc/kubernetes/kubeadm-config.yaml
      when: not cluster_admin_conf.stat.exists

    # 1. BOOTSTRAP: Bring up Private VIP only (No leader election)
    - name: "Bootstrap: Deploy Kube-VIP (Bootstrap Mode)"
      template:
        src: roles/k8s_prep/templates/kube-vip-bootstrap.yaml.j2
        dest: /etc/kubernetes/manifests/kube-vip.yaml
      when: not cluster_admin_conf.stat.exists

    # 2. INIT: Cluster initializes using Private VIP
    - name: Initialize Cluster (Skip Kube-Proxy)
      command: >
        kubeadm init --config /etc/kubernetes/kubeadm-config.yaml --upload-certs --skip-phases=addon/kube-proxy
      args:
        creates: /etc/kubernetes/admin.conf
      when: not cluster_admin_conf.stat.exists
      register: kubeadm_init_result

    # 3. HA: Deploy BOTH Private and Admin VIPs
    # The cluster keeps using Private. You start using Admin.
    - name: "HA: Deploy Dual Kube-VIP (Admin + Internal)"
      template:
        src: roles/k8s_prep/templates/kube-vip-ha.yaml.j2
        dest: "/etc/kubernetes/manifests/kube-vip-{{ item.name }}.yaml"
      loop:
        - { name: "admin",    vip: "{{ networks.admin.prefix }}.{{ cluster_vip }}",          interface: "{{ net_admin_dev }}",   metrics_port: "2112" }
        - { name: "internal", vip: "{{ networks.private.prefix }}.{{ cluster_private_vip }}",  interface: "{{ net_private_dev }}", metrics_port: "2113" }

    # 4. CLEANUP: Remove the bootstrap pod
    - name: "HA: Remove Bootstrap Kube-VIP Manifest"
      file:
        path: /etc/kubernetes/manifests/kube-vip.yaml
        state: absent

    - name: Setup Kubeconfig
      shell: |
        mkdir -p /home/{{ ansible_user }}/.kube
        cp /etc/kubernetes/admin.conf /home/{{ ansible_user }}/.kube/config
        chown -R {{ ansible_user }}:{{ ansible_user }} /home/{{ ansible_user }}/.kube

    # --- VERIFICATION ---
    - name: "Bootstrap: Verify Admin Access (Laptop -> Admin VIP)"
      wait_for:
        host: "{{ networks.admin.prefix }}.{{ cluster_vip }}"
        port: 6443
        timeout: 180
        delay: 5
      delegate_to: localhost
      become: no

    - name: "Bootstrap: Verify Internal Access (Node -> Internal VIP)"
      wait_for:
        host: "{{ networks.private.prefix }}.{{ cluster_private_vip }}"
        port: 6443
        timeout: 60
        delay: 2

    - name: Generate Join Token
      command: kubeadm token create --print-join-command
      register: join_command_raw

    - name: Get Certificate Key
      shell: kubeadm init phase upload-certs --upload-certs | tail -1
      register: cert_key_raw

    - name: Register Dummy Host for Variables
      add_host:
        name: "K8S_TOKEN_HOLDER"
        join_base: "{{ join_command_raw.stdout | trim }}"
        cert_key: "{{ cert_key_raw.stdout | trim }}"

    - name: "CNI: Deploy Cilium Network Fabric"
      include_role:
        name: k8s_addons
        tasks_from: cni_only

# ----------------------------------------------------------------------
# PHASE 3: JOIN THE CLUSTER
# ----------------------------------------------------------------------
- hosts: masters[1:]
  become: yes
  serial: 1
  vars:
    join_base: "{{ hostvars['K8S_TOKEN_HOLDER']['join_base'] }}"
    cert_key: "{{ hostvars['K8S_TOKEN_HOLDER']['cert_key'] }}"
  tasks:
    - include_role:
        name: networking
        tasks_from: normalize

    - name: Check if joined
      stat: path=/etc/kubernetes/kubelet.conf
      register: joined

    # 1. DEPLOY BOOTSTRAP KUBE-VIP (Private Only)
    - name: "Bootstrap: Deploy Kube-VIP (Bootstrap Mode)"
      template:
        src: roles/k8s_prep/templates/kube-vip-bootstrap.yaml.j2
        dest: /etc/kubernetes/manifests/kube-vip.yaml
      when: not joined.stat.exists

    # 2. JOIN CLUSTER
    - name: Join Cluster
      command: >
        {{ join_base }}
        --control-plane
        --certificate-key {{ cert_key }}
        --apiserver-advertise-address {{ private_ip }}
      when: not joined.stat.exists
      register: join_result

    # 3. HA: Deploy BOTH VIPs
    - name: "HA: Deploy Dual Kube-VIP (Admin + Internal)"
      template:
        src: roles/k8s_prep/templates/kube-vip-ha.yaml.j2
        dest: "/etc/kubernetes/manifests/kube-vip-{{ item.name }}.yaml"
      loop:
        - { name: "admin",    vip: "{{ networks.admin.prefix }}.{{ cluster_vip }}",          interface: "{{ net_admin_dev }}",   metrics_port: "2112" }
        - { name: "internal", vip: "{{ networks.private.prefix }}.{{ cluster_private_vip }}",  interface: "{{ net_private_dev }}", metrics_port: "2113" }

    # 4. CLEANUP
    - name: "HA: Remove Bootstrap Kube-VIP Manifest"
      file:
        path: /etc/kubernetes/manifests/kube-vip.yaml
        state: absent

    - name: "Stability: Pause for Etcd Sync"
      pause:
        seconds: 10
      when: join_result.changed

- hosts: workers
  become: yes
  serial: 2
  tasks:
    - include_role:
        name: networking
        tasks_from: normalize

    - name: Check if Node is already joined
      stat: path=/etc/kubernetes/kubelet.conf
      register: worker_joined

    - name: Fetch Join Command from Master
      command: kubeadm token create --print-join-command
      register: join_command_fetched
      delegate_to: "{{ groups['masters'][0] }}"
      run_once: true
      when: not worker_joined.stat.exists

    - name: Join Cluster
      command: >
        {{ join_command_fetched.stdout | trim }}
        --apiserver-advertise-address {{ private_ip }}
      when: not worker_joined.stat.exists

# ----------------------------------------------------------------------
# PHASE 4: NETWORK & ADDONS
# ----------------------------------------------------------------------
- hosts: masters[0]
  become: yes
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf

  pre_tasks:
    - name: "Cluster: Wait for all nodes to be registered"
      shell: "kubectl get nodes --no-headers | wc -l"
      register: node_count
      until: node_count.stdout | int == groups['k8s_cluster'] | length
      retries: 60
      delay: 10
      changed_when: false

    - name: "Taint: Ensure Masters are Schedulable"
      kubernetes.core.k8s_taint:
        state: absent
        name: "{{ item }}"
        taints:
          - key: "node-role.kubernetes.io/control-plane"
            effect: "NoSchedule"
      loop: "{{ groups['masters'] }}"
      when: hostvars[item].k8s_schedulable | default(true) | bool
    - name: "Label: Allow Masters to host Load Balancers"
      kubernetes.core.k8s:
        state: patched
        kind: Node
        name: "{{ item }}"
        definition:
          metadata:
            labels:
              # Setting the value to null removes the label
              node.kubernetes.io/exclude-from-external-load-balancers: null
      loop: "{{ groups['masters'] }}"
      when: hostvars[item].k8s_schedulable | default(true) | bool
  roles:
    - role: k8s_addons
      tags: k8s_addons